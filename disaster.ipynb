{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "disaster.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMeyw2Bo8eNLHqDCQ9vyDTR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gail529/Text-Classification-/blob/master/disaster.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMKiGOthUtfb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "1e4a456f-1a71-4136-f4e3-165e377ebce2"
      },
      "source": [
        "#getting the relevant tweets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "import re \n",
        "import string\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXNSaBvWWCCF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data=pd.read_csv('/content/train (1).csv')"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wS6POage8-il",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1396b5f9-e4f0-4d4f-8907-34d3f70bce5a"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7613, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NMEKcKgYrOk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#preprocessing all the songs\n",
        "for i in range(0,len(data)):\n",
        "    tweet=data.iloc[i,3]\n",
        "    #removing numbers\n",
        "    text=re.sub(r'\\d+','',tweet)\n",
        " \n",
        "    #lowercasing\n",
        "    text_lower=text.lower()\n",
        " \n",
        "    #punctuation removal \n",
        "    def  remove_punctuation(word): #function will take in the lowercased text and then remove all the punctuation\n",
        "        translator=str.maketrans('','',string.punctuation)\n",
        "        text_punct=word.translate(translator)\n",
        "        return text_punct\n",
        "    result=remove_punctuation(text_lower) #storing the resulting body of  text in variable result\n",
        " \n",
        "    #removing stopwords \n",
        "    def remove_stopwords(word):#The function will tokenize the result and then filter off  the stop_words \n",
        "        stop_words=set(stopwords.words(\"english\")) \n",
        "        word_tokens= word_tokenize(word)\n",
        "        result=[word for word in word_tokens if word not in stop_words] \n",
        "        return result\n",
        "    preprocessed_tweet=remove_stopwords(result)\n",
        " \n",
        "    #replacing the values of the lyrics column in the dataframe with the  preprocessed  lyrics\n",
        "    data.iat[i,3]=preprocessed_tweet"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKeKrXBXGSgb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample=data.tail(20)\n",
        "sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7mgbGda-Rgh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train,x_test,y_train,y_test=model_selection.train_test_split(data['text'],data['target'],test_size=0.3)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVzT9HyX_S3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x=data['text'].values"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDYjYIw7-_Q5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dummy_fun(doc):\n",
        "    return doc\n",
        "\n",
        "#vectorizing the text\n",
        "tfidf=TfidfVectorizer(\n",
        "    analyzer='word',\n",
        "    tokenizer=dummy_fun,\n",
        "    preprocessor=dummy_fun,\n",
        "    token_pattern=None)\n",
        "\n",
        "tfidf.fit(x)\n",
        "x_train_vectors=tfidf.transform(x_train)\n",
        "x_test_vectors=tfidf.transform(x_test)\n"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lP9phjhzbnMM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e14924c3-badf-4455-f209-95d4fed53d01"
      },
      "source": [
        "from sklearn import model_selection,svm\n",
        "SVM=svm.SVC(C=1.0,kernel='linear')\n",
        "SVM.fit(x_train_vectors,y_train)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
              "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "    tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1t9wWK24xpa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions=SVM.predict(x_test_vectors)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3YlSQv0Bnb4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "d7308619-5689-4417-a0bf-df9ad6c4e1a0"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test,predictions))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.89      0.83      1312\n",
            "           1       0.82      0.67      0.73       972\n",
            "\n",
            "    accuracy                           0.80      2284\n",
            "   macro avg       0.80      0.78      0.78      2284\n",
            "weighted avg       0.80      0.80      0.79      2284\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TD8n47tcllK_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "06fa4ed9-e4ca-423e-e3fc-1a16c675adfb"
      },
      "source": [
        "from sklearn import model_selection,naive_bayes\n",
        "nb=naive_bayes.MultinomialNB()\n",
        "nb.fit(x_train_vectors,y_train)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekc_ZLe08Zg_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb_Predictions=nb.predict(x_test_vectors)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCIELF7ADRw9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "c81e1f7d-ca08-4d7f-fee8-eb55530316e6"
      },
      "source": [
        "print(classification_report(y_test,nb_Predictions))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.91      0.84      1312\n",
            "           1       0.85      0.66      0.74       972\n",
            "\n",
            "    accuracy                           0.81      2284\n",
            "   macro avg       0.82      0.79      0.79      2284\n",
            "weighted avg       0.81      0.81      0.80      2284\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}